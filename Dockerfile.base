# syntax=docker/dockerfile:1.7

# Z-Image Turbo - Base image (deps + modele pre-charge)
# Objectif: ne reconstruire/pusher cette image que rarement.

# All ARGs must be declared before any FROM for multi-stage builds
ARG MODEL_SRC_IMAGE=swarzox/z-image-turbo-base:latest
# PyTorch 2.7.1 for TorchAO FP8 compatibility (2.8.0 has issues with TorchAO 0.14.1/0.15.0)
ARG BASE_IMAGE=pytorch/pytorch:2.7.1-cuda12.6-cudnn9-runtime

FROM ${MODEL_SRC_IMAGE} AS model_src

FROM ${BASE_IMAGE} AS base

LABEL maintainer="z-image-turbo"
LABEL description="Z-Image Turbo base image (deps + model) for Vast.ai Serverless"

ARG CACHE_DIT_REF=main
# flash-attn 2.7.4.post1 is more stable with PyTorch 2.7.x
ARG FLASH_ATTN_VERSION=2.7.4.post1

ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Tooling needed at runtime:
# - git: install diffusers/cache-dit from GitHub
# - gcc/g++: torch.compile (Inductor) builds C++ extensions at runtime
# - curl: required by vastai-sdk for fetching public keys
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    git \
    gcc \
    g++ \
    curl \
  && rm -rf /var/lib/apt/lists/*

COPY docker-requirements.txt /tmp/requirements.txt
RUN pip uninstall -y vastai 2>/dev/null || true && \
    pip install --no-cache-dir -r /tmp/requirements.txt && \
    pip install --no-cache-dir git+https://github.com/huggingface/diffusers.git && \
    pip install --no-cache-dir "git+https://github.com/vipshop/cache-dit.git@${CACHE_DIT_REF}" --no-deps && \
    pip install --no-cache-dir "vastai-sdk==0.3.1" --no-deps && \
    rm /tmp/requirements.txt

ENV HF_HUB_ENABLE_HF_TRANSFER=1

# Install FlashAttention-2 from a prebuilt wheel (no NVCC build).
RUN <<'BASH'
set -eu
export FLASH_ATTN_VERSION="${FLASH_ATTN_VERSION}"
python - <<'PY'
import os
import sys
import urllib.request
import subprocess

import torch

version = os.environ.get("FLASH_ATTN_VERSION", "2.8.3").strip()
py_tag = f"cp{sys.version_info.major}{sys.version_info.minor}"

torch_version = (torch.__version__ or "").split("+", 1)[0]
torch_parts = torch_version.split(".")
torch_mm = ".".join(torch_parts[:2]) if len(torch_parts) >= 2 else torch_version

cuda_version = torch.version.cuda or ""
cuda_major = cuda_version.split(".", 1)[0] if cuda_version else "12"
cu_tag = f"cu{cuda_major}"

cxx11_abi = "TRUE" if getattr(torch._C, "_GLIBCXX_USE_CXX11_ABI", False) else "FALSE"

wheel_name = (
    f"flash_attn-{version}+{cu_tag}torch{torch_mm}"
    f"cxx11abi{cxx11_abi}-{py_tag}-{py_tag}-linux_x86_64.whl"
)
url = f"https://github.com/Dao-AILab/flash-attention/releases/download/v{version}/{wheel_name}"
dest_path = f"/tmp/{wheel_name}"

print(f"[flash-attn] torch={torch.__version__} cuda={torch.version.cuda} py={sys.version.split()[0]} cxx11abi={cxx11_abi}")
print(f"[flash-attn] Wheel URL: {url}")

req = urllib.request.Request(url, headers={"User-Agent": "z-image-turbo"})
with urllib.request.urlopen(req, timeout=300) as resp:
    data = resp.read()

with open(dest_path, "wb") as f:
    f.write(data)

print(f"[flash-attn] Downloaded wheel: {dest_path} ({len(data)} bytes)")
subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", dest_path])
try:
    os.remove(dest_path)
except OSError:
    pass

import flash_attn  # noqa: F401
print(f"[flash-attn] installed: {getattr(flash_attn, '__version__', 'unknown')}")
PY
BASH

# torch.compile (Inductor/Triton) needs a C/C++ compiler at runtime.
# We install GCC via apt-get above; `ninja` already exists in the base image.

RUN mkdir -p /app /var/log/zimage

WORKDIR /app

# === Target: copy the model from an existing image (no HF token) ===
FROM base AS base_copy
COPY --from=model_src /models /models
COPY --from=model_src /app/z-image-turbo /app/z-image-turbo

# === Target: download the model during build (HF token) ===
FROM base AS base_download

RUN --mount=type=secret,id=hf_token,required=false <<'BASH'
set -eu

if [ ! -f /run/secrets/hf_token ]; then
  echo "Missing build secret: hf_token (use build-base with SKIP_MODEL_DOWNLOAD=1 to copy the model)." >&2
  exit 1
fi

mkdir -p /models /app/z-image-turbo
export HF_TOKEN="$(cat /run/secrets/hf_token)"
export HUGGINGFACE_HUB_TOKEN="$HF_TOKEN"

python - <<'PY'
import os
from huggingface_hub import snapshot_download

token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")
if not token:
    raise RuntimeError("HF_TOKEN is required to download the model during build.")

snapshot_download(
    repo_id="Tongyi-MAI/Z-Image-Turbo",
    cache_dir="/models",
    local_dir="/app/z-image-turbo",
    local_dir_use_symlinks=True,
    token=token,
)
print("Downloaded Tongyi-MAI/Z-Image-Turbo into /app/z-image-turbo.")
PY
BASH
